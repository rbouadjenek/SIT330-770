---
type: lecture
week: Week 10
date: 2024-05-13T10:00:00
title: Transformers and Pretrained LMs
tldr: "Transformers and Pretrained LMs."
hide_from_announcments: true
thumbnail: /static_files/presentations/transformers.jpg
links: 
    - url: /static_files/presentations/Week_10_-_Transformers_and_Pretrained_LMs.pdf
      name: slides
    - url: /static_files/presentations/Week_10_-_Transformers_and_Pretrained_LMs_6up.pdf
      name: slides 6up
---
**Video recordings (X Hour, XX Minutes and XX Seconds):**
- Transformers: Attention Is All You Need! (1 Hour, 10 Minutes and 29 Seconds)
    - [Introduction to Transformers (16:23)](https://youtu.be/KCqihbmWeao)
    - [Self-Attention Mechanism (18:10)](https://youtu.be/qEBFfTywJNg)
    - [The Encoder Transformer Block (10:27)](https://youtu.be/iFD27h617jo)
    - [The Input: Embeddings for Tokens (8:03)](https://youtu.be/DZuZFPH5lbo)
    - [The Input: Embeddings for Positions (11:15)](https://youtu.be/dRQ8cDMbq9E)
    - [The Task Specific Head (6:11)](https://youtu.be/Ek6W2Wd7Ty4)
- Pre-trained LMs (X Hour, XX Minutes and XX Seconds)
    - [BERT: Bidirectional Encoder Representations from Transformers (13:34)](https://youtu.be/7QRpWx9UhWo)
    - [BERT pre-training (13:39)](https://youtu.be/CJsVqr5uWvc)
    - [BERT fine-tuning (9:16)](https://youtu.be/yI6CfB_CceY)
    - [BERT Performance (6:11)](https://youtu.be/t46q_h3IC6E)
    - Other Models Based on Transformers
    - HuggingFace
